# ============================================================================
# ReasonBorn Pre-training Configuration — Optimized for AMD MI300X
# Target A: 1x MI300X (192 GB HBM3) → batch_size 4096
# Target B: 8x MI300X (1.5 TB HBM3) → effective_batch_size 32768
# ============================================================================

num_epochs: 1
sequence_length: 2048
gradient_clipping: 1.0

# --- Batch Size (tuned for 192GB VRAM per MI300X) ---
# Single MI300X can fit ~4096 batch with bf16 for a 500M param model
batch_size: 4096
gradient_accumulation_steps: 2
# 8x MI300X: 4096 * 2 * 8 = 65536 effective batch (or reduce accum for speed)
effective_batch_size: 65536

optimizer:
  type: "adamw"
  learning_rate: 3.0e-4
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  eps: 1.0e-8

lr_scheduler:
  type: "cosine_with_warmup"
  warmup_steps: 4000
  max_steps: 500000
  min_lr: 3.0e-5

# bf16 is natively supported on MI300X (HBM3 matrix cores)
mixed_precision: "bf16"

loss_weights:
  mlm: 1.0
  contrastive: 0.1
  verification: 0.05

# --- ROCm-specific ---
rocm:
  target_arch: "gfx942"
  hip_alloc_conf: "expandable_segments:True"
