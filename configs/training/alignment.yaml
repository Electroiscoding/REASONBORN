# ReasonBorn Training Configuration - Alignment Phase
vocab_size: 50000
d_model: 768
num_layers: 18
num_heads: 12
intermediate_size: 3072
max_depth: 5
e_cap: 10000
s_cap: 1000000
policy_hash: "default_v1_hash"
moe_expert_layers: [6, 7, 8, 9, 10, 11]
num_experts: 8
top_k: 2
load_balance_loss_weight: 0.01

# Training Hyperparameters
batch_size: 256
learning_rate: 3.0e-4
warmup_steps: 4000
max_steps: 500000
bf16: true
gradient_accumulation_steps: 4
device: "cuda"
C_clip: 1.0
Ïƒ_noise: 1.1
