# ReasonBorn Training Configuration - Alignment Phase
vocab_size: 50000
d_model: 768
num_layers: 18
num_heads: 12
intermediate_size: 3072
max_depth: 5
e_cap: 10000
s_cap: 1000000
policy_hash: "default_v1_hash"
moe_expert_layers: [6, 7, 8, 9, 10, 11]
num_experts: 8
top_k: 2
load_balance_loss_weight: 0.01

# Training Hyperparameters (optimized for MI300X 192GB HBM3)
batch_size: 1024
learning_rate: 3.0e-4
warmup_steps: 4000
max_steps: 500000
bf16: true
gradient_accumulation_steps: 4
# PyTorch on ROCm uses "cuda" device string — HIP backend is selected transparently
device: "cuda"
C_clip: 1.0
σ_noise: 1.1
